
# Create a modular project structure and write files to /mnt/data/medi_safe_modular
import os, textwrap, json, io, pandas as pd, sys, shutil

root = "/mnt/data/medi_safe_modular"
if os.path.exists(root):
    shutil.rmtree(root)
os.makedirs(root, exist_ok=True)
os.makedirs(os.path.join(root, "ddi_app"), exist_ok=True)

# ---------- requirements ----------
requirements = """\
streamlit
langgraph>=0.2.34
neo4j>=5.20.0
pandas
python-dotenv
httpx
filelock
openai>=1.40.0
pydantic>=2.7.0
beautifulsoup4
lxml
tenacity
aiosqlite
"""
open(os.path.join(root, "requirements.txt"), "w").write(requirements)

# ---------- .env sample ----------
env_sample = """\
# Fill these before running `streamlit run app.py`
OPENAI_API_KEY=sk-...
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j
DB_CSV=./DB.csv
"""
open(os.path.join(root, ".env.example"), "w").write(env_sample)

# ---------- ddi_app/config.py ----------
config_py = '''\
import os
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

DB_CSV = os.getenv("DB_CSV", "./DB.csv")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "neo4j")

openai_client = OpenAI(api_key=OPENAI_API_KEY)
'''
open(os.path.join(root, "ddi_app", "config.py"), "w").write(config_py)

# ---------- ddi_app/db_csv.py ----------
db_csv_py = '''\
import os
import re
from datetime import datetime
from typing import Dict, Any, List
import pandas as pd
from filelock import FileLock
from .config import DB_CSV

COLUMN_ALIASES = {
    "drug": ["drug", "drug_name", "ì œí’ˆëª…", "ì•½í’ˆëª…"],
    "component": ["component", "components", "ì„±ë¶„", "ì„±ë¶„1", "ì„±ë¶„_ë¦¬ìŠ¤íŠ¸"],
    "partner": ["partner", "ìƒëŒ€ì•½ë¬¼", "ìƒí˜¸ì‘ìš©ìƒëŒ€", "ì œí’ˆëª…2", "ìƒëŒ€"],
    "interaction": ["interaction", "ìƒí˜¸ì‘ìš©", "ì„¤ëª…", "ì‚¬ìœ "],
    "severity": ["severity", "ë“±ê¸‰", "ê²°ê³¼"],
    "source": ["source", "ì¶œì²˜"],
    "updated_at": ["updated_at", "ì—…ë°ì´íŠ¸", "ìˆ˜ì •ì¼"],
    "confidence": ["confidence"],
    "evidence": ["evidence", "ê·¼ê±°"],
}

REQUIRED_CANONICAL = [
    "drug","component","partner","interaction","severity",
    "source","updated_at","confidence","evidence",
]

def _canonicalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    mapping = {}
    lower_cols = {c.lower(): c for c in df.columns}
    for canon, aliases in COLUMN_ALIASES.items():
        for a in aliases:
            if a.lower() in lower_cols:
                mapping[lower_cols[a.lower()]] = canon
                break
    df = df.rename(columns=mapping)
    for c in REQUIRED_CANONICAL:
        if c not in df.columns:
            df[c] = None
    return df[REQUIRED_CANONICAL]

def read_db() -> pd.DataFrame:
    if not os.path.exists(DB_CSV):
        return pd.DataFrame(columns=REQUIRED_CANONICAL)
    with FileLock(DB_CSV + ".lock"):
        df = pd.read_csv(DB_CSV)
    df = _canonicalize_columns(df)
    return df

def write_db(df: pd.DataFrame) -> None:
    df = _canonicalize_columns(df)
    with FileLock(DB_CSV + ".lock"):
        df.to_csv(DB_CSV, index=False)

def search_db_for_drug(df: pd.DataFrame, name: str) -> pd.DataFrame:
    if not name:
        return df.iloc[0:0]
    pat = re.escape(name.lower())
    mask = df["drug"].fillna("").str.lower().str.contains(pat)
    mask_partner = df["partner"].fillna("").str.lower().str.contains(pat)
    return df[mask | mask_partner]

def get_monograph(df: pd.DataFrame, name: str) -> Dict[str, Any]:
    subset = search_db_for_drug(df, name)
    comps = sorted({c for c in subset["component"].dropna().astype(str).tolist() if c})
    interactions = subset.to_dict(orient="records")
    return {"drug": name, "components": comps, "interactions": interactions}

def upsert_interaction(
    df: pd.DataFrame,
    drug: str,
    component: str,
    partner: str,
    interaction: str,
    severity: str = "",
    source: str = "",
    confidence: float = 0.5,
    evidence: str = "",
) -> pd.DataFrame:
    now = datetime.utcnow().isoformat()
    row = {
        "drug": drug,
        "component": component,
        "partner": partner,
        "interaction": interaction,
        "severity": severity,
        "source": source,
        "updated_at": now,
        "confidence": confidence,
        "evidence": evidence,
    }
    return pd.concat([df, pd.DataFrame([row])], ignore_index=True)
'''
open(os.path.join(root, "ddi_app", "db_csv.py"), "w").write(db_csv_py)

# ---------- ddi_app/web_retrieval.py ----------
web_retrieval_py = '''\
import httpx
from bs4 import BeautifulSoup
from pydantic import BaseModel
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from typing import Optional, List, Dict, Tuple
from .db_csv import upsert_interaction
from .config import DB_CSV
import pandas as pd

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
)
DEFAULT_HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ko,en;q=0.9"}

class WebExtract(BaseModel):
    drug: str
    components: List[str] = []
    partner: Optional[str] = None
    interaction: Optional[str] = None
    severity: Optional[str] = None
    source: Optional[str] = None
    evidence: Optional[str] = None
    confidence: float = 0.7

class WebFetchError(Exception):
    pass

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.6, min=0.5, max=4),
       reraise=True, retry=retry_if_exception_type(WebFetchError))
async def _fetch_text(client: httpx.AsyncClient, url: str) -> str:
    try:
        r = await client.get(url, timeout=15)
        r.raise_for_status()
        return r.text
    except Exception as e:
        raise WebFetchError(str(e))

async def fetch_health_kr(drug: str) -> List[WebExtract]:
    base = "https://www.health.kr/searchDrug"
    search_url = f"{base}/search_result.asp?drug_name={drug}"
    out: List[WebExtract] = []
    async with httpx.AsyncClient(headers=DEFAULT_HEADERS, follow_redirects=True) as ac:
        html = await _fetch_text(ac, search_url)
        soup = BeautifulSoup(html, "lxml")
        detail_candidates = []
        for a in soup.select('a[href*="result_drug.asp?drug_cd="]'):
            href = a.get("href")
            if href and href not in detail_candidates:
                detail_candidates.append(base + "/" + href.lstrip("/"))
        for url in detail_candidates[:2]:
            try:
                dhtml = await _fetch_text(ac, url)
            except Exception:
                continue
            dsoup = BeautifulSoup(dhtml, "lxml")
            name = dsoup.select_one(".tit_area h2, h1, .title")
            dname = name.get_text(strip=True) if name else drug
            comp_nodes = dsoup.select(".ingre, .comp, .table td, li")
            comps = []
            for n in comp_nodes:
                t = n.get_text(" ", strip=True)
                if any(k in t for k in ["mg", "ì •", "ìº¡ìŠ", "ì„±ë¶„"]):
                    comps.append(t)
            comps = sorted(list({c for c in comps if len(c) < 120}))[:10]

            inter_url = None
            for a in dsoup.select('a[href*="result_interaction.asp?drug_cd="]'):
                inter_url = base + "/" + a.get("href").lstrip("/")
                break
            inter_desc = None
            if inter_url:
                try:
                    ihtml = await _fetch_text(ac, inter_url)
                    isoup = BeautifulSoup(ihtml, "lxml")
                    paras = [p.get_text(" ", strip=True) for p in isoup.select(".cnt, .contents, p, li")]
                    inter_desc = "; ".join(paras[:6]) if paras else None
                except Exception:
                    pass

            out.append(WebExtract(
                drug=dname or drug,
                components=comps,
                interaction=inter_desc,
                source="health.kr",
                evidence=(inter_url or url),
                confidence=0.75 if inter_desc else 0.65
            ))
    return out

async def fetch_ddinter(drug: str, partner: Optional[str]) -> List[WebExtract]:
    base = "https://ddinter.scbdd.com/ddinter"
    out: List[WebExtract] = []
    async with httpx.AsyncClient(headers=DEFAULT_HEADERS, follow_redirects=True, verify=False) as ac:
        try:
            if partner:
                url = f"{base}/drug-detail/DDInter14/"
                html = await _fetch_text(ac, url)
                soup = BeautifulSoup(html, "lxml")
                paras = [p.get_text(" ", strip=True) for p in soup.select("p, li")]
                desc = "; ".join(paras[:6]) if paras else None
                out.append(WebExtract(drug=drug, partner=partner, interaction=desc, source="ddinter", evidence=url, confidence=0.6))
            else:
                url = f"{base}/drug-list/"
                html = await _fetch_text(ac, url)
                soup = BeautifulSoup(html, "lxml")
                comps = [li.get_text(" ", strip=True) for li in soup.select("li")][:5]
                out.append(WebExtract(drug=drug, components=comps, source="ddinter", evidence=url, confidence=0.55))
        except Exception:
            return []
    return out

def score_confidence(source: str, base: float | None = None) -> float:
    if base is None:
        base = 0.6
    s = (source or "").lower()
    if s in {"db", "csv", "local"}:
        return max(base, 0.9)
    if s in {"health.kr", "drugbank", "hira", "fda", "korea mfds"}:
        return max(base, 0.8)
    if s in {"ddinter"}:
        return max(base, 0.65)
    if s in {"llm"}:
        return max(base, 0.5)
    return base

async def web_retrieve(drug: str, partner: Optional[str]) -> List[WebExtract]:
    tasks = [fetch_health_kr(drug)]
    if partner:
        tasks.append(fetch_ddinter(drug, partner))
    results = await httpx.AsyncClient().__aexit__  # dummy to satisfy type checkers (no-op at runtime)
    results = await __import__("asyncio").gather(fetch_health_kr(drug), *( [fetch_ddinter(drug, partner)] if partner else [] ), return_exceptions=True)
    flat: List[WebExtract] = []
    for r in results:
        if isinstance(r, Exception):
            continue
        flat.extend(r)
    best: Dict[Tuple[str, Optional[str]], WebExtract] = {}
    for item in flat:
        key = (item.drug.lower(), item.partner.lower() if item.partner else None)
        if key not in best or (item.confidence or 0) > (best[key].confidence or 0):
            best[key] = item
    return list(best.values())
'''
open(os.path.join(root, "ddi_app", "web_retrieval.py"), "w").write(web_retrieval_py)

# ---------- ddi_app/llm_backoff.py ----------
llm_backoff_py = '''\
import json
import asyncio
from typing import Optional, List
from pydantic import BaseModel, Field
from .config import openai_client

class DDIExtract(BaseModel):
    drug: str = Field(..., description="Drug name")
    components: List[str] = Field(default_factory=list)
    partner: Optional[str] = Field(None, description="Other drug if a pair query")
    interaction: Optional[str] = None
    severity: Optional[str] = None
    source: Optional[str] = None
    evidence: Optional[str] = None
    confidence: Optional[float] = 0.5

async def llm_extract_ddi(drug: str, partner: Optional[str]) -> List[DDIExtract]:
    system = "You are a clinical pharmacist. Return concise, factual JSON. Include `source` and short `evidence` text when you can."
    user = (f"Extract components and, if partner is given, the DDI between {drug}" +
            (f" and {partner}." if partner else ".") +
            " Return JSON array of objects with keys: drug, components, partner, interaction, severity, source, evidence, confidence.")

    def _call():
        resp = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
            response_format={"type": "json_object"},
        )
        return resp.choices[0].message.content

    content = await asyncio.to_thread(_call)
    try:
        data = json.loads(content)
        arr = data if isinstance(data, list) else data.get("items") or data.get("data") or []
    except Exception:
        arr = []
    out: List[DDIExtract] = []
    for item in arr:
        try:
            out.append(DDIExtract(**item))
        except Exception:
            continue
    if not out:
        out.append(DDIExtract(drug=drug, partner=partner, components=[], interaction=None))
    return out
'''
open(os.path.join(root, "ddi_app", "llm_backoff.py"), "w").write(llm_backoff_py)

# ---------- ddi_app/neo4j_utils.py ----------
neo4j_utils_py = '''\
from typing import List
from neo4j import GraphDatabase

class Neo4jClient:
    def __init__(self, uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()

    def ensure_constraints(self):
        with self.driver.session() as s:
            s.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Drug) REQUIRE d.name IS UNIQUE")
            s.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Component) REQUIRE c.name IS UNIQUE")

    def merge_components(self, drug: str, components: List[str]):
        if not components:
            return
        q = (
            "MERGE (d:Drug {name:$drug})\\n"
            "WITH d\\n"
            "UNWIND $components AS cname\\n"
            "MERGE (c:Component {name:cname})\\n"
            "MERGE (d)-[:HAS_COMPONENT]->(c)"
        )
        with self.driver.session() as s:
            s.run(q, drug=drug, components=components)

    def merge_ddi(self, a: str, b: str, interaction: str, severity: str, source: str, confidence: float, evidence: str):
        q = (
            "MERGE (a:Drug {name:$a})\\n"
            "MERGE (b:Drug {name:$b})\\n"
            "MERGE (a)-[r:INTERACTS_WITH]->(b)\\n"
            "ON CREATE SET r.first_seen = timestamp()\\n"
            "SET r.last_seen = timestamp(), r.description=$desc, r.severity=$sev, "
            "r.source=$src, r.confidence=$conf, r.evidence=$evid"
        )
        q2 = q.replace("(a)-[r:INTERACTS_WITH]->(b)", "(b)-[r:INTERACTS_WITH]->(a)")
        with self.driver.session() as s:
            s.run(q, a=a, b=b, desc=interaction, sev=severity, src=source, conf=float(confidence or 0), evid=evidence)
            s.run(q2, a=a, b=b, desc=interaction, sev=severity, src=source, conf=float(confidence or 0), evid=evidence)
'''
open(os.path.join(root, "ddi_app", "neo4j_utils.py"), "w").write(neo4j_utils_py)

# ---------- ddi_app/ui.py ----------
ui_py = '''\
from typing import Dict, Any, List, Optional

def monograph_md(m: Dict[str, Any]) -> str:
    comps = ", ".join(m.get("components", []) or []) or "ì •ë³´ ì—†ìŒ"
    return (
        f"**êµ¬ì„± ì„±ë¶„:** {comps}\\n\\n"
        f"**DB ë‚´ ìƒí˜¸ì‘ìš© ë ˆì½”ë“œ ìˆ˜:** {len(m.get('interactions', []))}\\n"
    )

def format_answer(drug1: str, mono1: Dict[str, Any], drug2: Optional[str], mono2: Optional[Dict[str, Any]], ddi: Optional[Dict[str, Any]]) -> str:
    sections: List[str] = []
    sections.append(f"### ğŸ“Œ ì•½ë¬¼ 1: {drug1}\\n\\n" + monograph_md(mono1 or {}))
    if drug2 and mono2:
        sections.append(f"### ğŸ“Œ ì•½ë¬¼ 2: {drug2}\\n\\n" + monograph_md(mono2))

    if drug2:
        sections.append("### ğŸ’¥ ë‘ ì•½ë¬¼ì˜ ìƒí˜¸ì‘ìš©")
        if ddi and ddi.get("interaction"):
            src = ddi.get("source","DB/LLM/WEB")
            conf = ddi.get("confidence")
            ev = ddi.get("evidence")
            sections.append(
                f"- âœ… **í•¨ê»˜ ë³µìš© ê°€ëŠ¥ ì—¬ë¶€/ì£¼ì˜:** {ddi.get('severity','ë¯¸ìƒ')}\\n"
                f"- â— **ìš”ì•½:** {ddi.get('interaction')}\\n"
                f"- ğŸ“š **ì¶œì²˜:** {src}  |  **ì‹ ë¢°ë„:** {conf if conf is not None else 'N/A'}\\n"
                + (f"- ğŸ” **ê·¼ê±°:** {ev}" if ev else "")
            )
        else:
            sections.append("- DB/ì›¹ì—ì„œ ëª…í™•í•œ ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœì‹  ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    return "\\n\\n".join(sections)
'''
open(os.path.join(root, "ddi_app", "ui.py"), "w").write(ui_py)

# ---------- ddi_app/pipeline.py ----------
pipeline_py = '''\
import io
import asyncio
import pandas as pd
from typing import Optional, Dict, Any
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from .db_csv import read_db, write_db, get_monograph, upsert_interaction, search_db_for_drug, REQUIRED_CANONICAL
from .web_retrieval import web_retrieve, score_confidence
from .llm_backoff import llm_extract_ddi
from .neo4j_utils import Neo4jClient
from .config import NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD
from .ui import format_answer

class AppState(BaseModel):
    drug1: str
    drug2: Optional[str] = None
    df_json: Optional[str] = None
    found_in_db: bool = False
    monograph1: Optional[Dict[str, Any]] = None
    monograph2: Optional[Dict[str, Any]] = None
    ddi: Optional[Dict[str, Any]] = None
    answer_md: Optional[str] = None

async def load_db_node(state: AppState) -> AppState:
    df = read_db()
    return state.copy(update={"df_json": df.to_json(orient="records")})

async def decide_source_node(state: AppState) -> AppState:
    df = pd.read_json(io.StringIO(state.df_json)) if state.df_json else pd.DataFrame(columns=REQUIRED_CANONICAL)
    has1 = not search_db_for_drug(df, state.drug1).empty
    has2 = (not search_db_for_drug(df, state.drug2).empty) if state.drug2 else True
    return state.copy(update={"found_in_db": bool(has1 and has2)})

async def fetch_from_db_node(state: AppState) -> AppState:
    df = pd.read_json(io.StringIO(state.df_json)) if state.df_json else pd.DataFrame(columns=REQUIRED_CANONICAL)
    mono1 = get_monograph(df, state.drug1)
    mono2 = get_monograph(df, state.drug2) if state.drug2 else None
    ddi = None
    if state.drug2:
        sub = df[\
            ((df["drug"].str.lower()==state.drug1.lower()) & (df["partner"].str.lower()==state.drug2.lower())) |\
            ((df["drug"].str.lower()==state.drug2.lower()) & (df["partner"].str.lower()==state.drug1.lower()))\
        ]
        if not sub.empty:
            row = sub.sort_values("updated_at", ascending=False).iloc[0].to_dict()
            ddi = {\
                "a": state.drug1,\
                "b": state.drug2,\
                "interaction": row.get("interaction"),\
                "severity": row.get("severity"),\
                "source": row.get("source"),\
                "confidence": float(row.get("confidence") or 0.9),\
                "evidence": row.get("evidence"),\
            }
    return state.copy(update={"monograph1": mono1, "monograph2": mono2, "ddi": ddi})

async def web_retrieve_node(state: AppState) -> AppState:
    import pandas as pd
    df = pd.read_json(io.StringIO(state.df_json)) if state.df_json else pd.DataFrame(columns=REQUIRED_CANONICAL)
    items = await web_retrieve(state.drug1, state.drug2)
    for ex in items:
        src = ex.source or "web"
        conf = score_confidence(src, ex.confidence)
        if ex.partner and ex.interaction:
            comp_list = ex.components or [""]
            for comp in comp_list:
                df = upsert_interaction(df, ex.drug or state.drug1, comp, ex.partner, ex.interaction or "", ex.severity or "", src, conf, ex.evidence or "")
        elif not state.drug2 and ex.components:
            for comp in ex.components:
                df = upsert_interaction(df, ex.drug or state.drug1, comp, "", "", "", src, conf, ex.evidence or "")
    write_db(df)
    mono1 = get_monograph(df, state.drug1)
    mono2 = get_monograph(df, state.drug2) if state.drug2 else None
    ddi = None
    if state.drug2:
        sub = df[\
            ((df["drug"].str.lower()==state.drug1.lower()) & (df["partner"].str.lower()==state.drug2.lower())) |\
            ((df["drug"].str.lower()==state.drug2.lower()) & (df["partner"].str.lower()==state.drug1.lower()))\
        ]
        if not sub.empty:
            row = sub.sort_values("confidence", ascending=False).iloc[0].to_dict()
            ddi = {\
                "a": state.drug1,\
                "b": state.drug2,\
                "interaction": row.get("interaction"),\
                "severity": row.get("severity"),\
                "source": row.get("source"),\
                "confidence": float(row.get("confidence") or 0.7),\
                "evidence": row.get("evidence"),\
            }
    return state.copy(update={"df_json": df.to_json(orient="records"), "monograph1": mono1, "monograph2": mono2, "ddi": ddi})

async def llm_backoff_node(state: AppState) -> AppState:
    import pandas as pd
    df = pd.read_json(io.StringIO(state.df_json)) if state.df_json else pd.DataFrame(columns=REQUIRED_CANONICAL)
    extracts = await llm_extract_ddi(state.drug1, state.drug2)
    for ex in extracts:
        comp_list = ex.components or [""]
        partner = ex.partner or (state.drug2 or "")
        if partner or ex.interaction:
            for comp in comp_list:
                df = upsert_interaction(df, ex.drug or state.drug1, comp, partner, ex.interaction or "", ex.severity or "", ex.source or "LLM", 0.5, ex.evidence or "")
    write_db(df)
    mono1 = get_monograph(df, state.drug1)
    mono2 = get_monograph(df, state.drug2) if state.drug2 else None
    ddi = None
    if state.drug2:
        sub = df[\
            ((df["drug"].str.lower()==state.drug1.lower()) & (df["partner"].str.lower()==state.drug2.lower())) |\
            ((df["drug"].str.lower()==state.drug2.lower()) & (df["partner"].str.lower()==state.drug1.lower()))\
        ]
        if not sub.empty:
            row = sub.sort_values("confidence", ascending=False).iloc[0].to_dict()
            ddi = {\
                "a": state.drug1,\
                "b": state.drug2,\
                "interaction": row.get("interaction"),\
                "severity": row.get("severity"),\
                "source": row.get("source"),\
                "confidence": float(row.get("confidence") or 0.5),\
                "evidence": row.get("evidence"),\
            }
    return state.copy(update={"df_json": df.to_json(orient="records"), "monograph1": mono1, "monograph2": mono2, "ddi": ddi})

async def write_neo4j_node(state: AppState) -> AppState:
    neo = Neo4jClient(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)
    try:
        neo.ensure_constraints()
        if state.monograph1 and state.monograph1.get("components"):
            neo.merge_components(state.drug1, state.monograph1["components"])
        if state.monograph2 and state.monograph2.get("components"):
            neo.merge_components(state.drug2, state.monograph2["components"])
        if state.drug2 and state.ddi and state.ddi.get("interaction"):
            neo.merge_ddi(state.ddi["a"], state.ddi["b"], state.ddi.get("interaction",""), state.ddi.get("severity",""),
                          state.ddi.get("source",""), float(state.ddi.get("confidence") or 0), state.ddi.get("evidence",""))
    finally:
        neo.close()
    return state

async def format_answer_node(state: AppState) -> AppState:
    md = format_answer(state.drug1, state.monograph1 or {}, state.drug2, state.monograph2, state.ddi)
    return state.copy(update={"answer_md": md})

def build_workflow() -> "StateGraph":
    workflow = StateGraph(AppState)
    workflow.add_node("load_db", load_db_node)
    workflow.add_node("decide_source", decide_source_node)
    workflow.add_node("fetch_from_db", fetch_from_db_node)
    workflow.add_node("web_retrieve", web_retrieve_node)
    workflow.add_node("llm_backoff", llm_backoff_node)
    workflow.add_node("write_neo4j", write_neo4j_node)
    workflow.add_node("format_answer", format_answer_node)

    workflow.set_entry_point("load_db")
    workflow.add_edge("load_db", "decide_source")
    workflow.add_conditional_edges("decide_source",
        lambda s: "from_db" if s.found_in_db else "need_web",
        {"from_db": "fetch_from_db", "need_web": "web_retrieve"})
    workflow.add_edge("web_retrieve", "llm_backoff")
    workflow.add_edge("fetch_from_db", "write_neo4j")
    workflow.add_edge("llm_backoff", "write_neo4j")
    workflow.add_edge("write_neo4j", "format_answer")
    workflow.add_edge("format_answer", END)
    return workflow
'''
open(os.path.join(root, "ddi_app", "pipeline.py"), "w").write(pipeline_py)

# ---------- app.py (Streamlit entry) ----------
app_py = '''\
import asyncio
from uuid import uuid4
import streamlit as st
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

from ddi_app.pipeline import build_workflow, AppState

st.set_page_config(page_title="ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ’Š ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë¶„ì„ê¸°")
st.markdown(\"\"\"\
#### ğŸ’¬ ì–´ë–¤ ì•½ë¬¼(ì•½í’ˆ)ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì„¸ìš”? ë˜ëŠ” ë‘ ì•½ë¬¼ì˜ ìƒí˜¸ê´€ê³„ë¥¼ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
- **í•œ ê°€ì§€ ì•½ë¬¼**ë§Œ ê¶ê¸ˆí•˜ì‹œë©´ ğŸ‘‰ ì™¼ìª½ ì…ë ¥ì¹¸ì—ë§Œ ì…ë ¥í•´ ì£¼ì„¸ìš”.  
- **ì•½ë¬¼ ê°„ ìƒí˜¸ì‘ìš©**ì´ ê¶ê¸ˆí•˜ë©´ ğŸ‘‰ ì˜¤ë¥¸ìª½ ì…ë ¥ì¹¸ë„ í•¨ê»˜ ì…ë ¥í•´ ì£¼ì„¸ìš”.
\"\"\")

col1, col2 = st.columns(2)
with col1:
    drug1 = st.text_input("ğŸ©º ì•½ë¬¼(ì•½í’ˆ) 1", placeholder="ì˜ˆ: íƒ€ì´ë ˆë†€").strip()
with col2:
    drug2 = st.text_input("ğŸ©º ì•½ë¬¼(ì•½í’ˆ) 2", placeholder="ì˜ˆ: ì´ë¶€í”„ë¡œíœ").strip()
btn = st.button("ğŸ” ë¶„ì„í•˜ê¸°")

# Session-scoped IDs
if "thread_id" not in st.session_state:
    st.session_state["thread_id"] = f"ui-{uuid4()}"
st.session_state.setdefault("checkpoint_ns", "streamlit-ddi")

# One-time compile with AsyncSqliteSaver (avoid multiple starts)
async def _ensure_compiled():
    if "graph_app" in st.session_state:
        return
    cm = AsyncSqliteSaver.from_conn_string("langgraph_checkpoints.db")
    saver = await cm.__aenter__()
    st.session_state["_cp_cm"] = cm
    workflow = build_workflow()
    st.session_state["graph_app"] = workflow.compile(checkpointer=saver)

asyncio.run(_ensure_compiled())

if btn:
    if not drug1:
        st.warning("âš ï¸ ì•½ë¬¼ 1ì€ ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()
    with st.spinner("ğŸ’¬ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
        init = AppState(drug1=drug1, drug2=(drug2 or None))
        app = st.session_state["graph_app"]
        final_state = asyncio.run(app.ainvoke(
            init,
            config={
                "configurable": {
                    "thread_id": st.session_state["thread_id"],
                    "checkpoint_ns": st.session_state["checkpoint_ns"],
                }
            },
        ))

    st.markdown("---")
    if drug2:
        try:
            parts = final_state.answer_md.split(f"### ğŸ“Œ ì•½ë¬¼ 2: {drug2}")
            left = parts[0]
            rest = f"### ğŸ“Œ ì•½ë¬¼ 2: {drug2}" + (parts[1] if len(parts) > 1 else "")
            right, *tail = rest.split("### ğŸ’¥ ë‘ ì•½ë¬¼ì˜ ìƒí˜¸ì‘ìš©")
            with col1:
                st.markdown(left, unsafe_allow_html=True)
            with col2:
                st.markdown(right, unsafe_allow_html=True)
            st.markdown("---")
            st.markdown("### ğŸ’¥ ë‘ ì•½ë¬¼ì˜ ìƒí˜¸ì‘ìš©\\n" + (tail[0] if tail else ""), unsafe_allow_html=True)
        except Exception:
            st.markdown(final_state.answer_md, unsafe_allow_html=True)
    else:
        with col1:
            st.markdown(final_state.answer_md, unsafe_allow_html=True)
'''
open(os.path.join(root, "app.py"), "w").write(app_py)

# Write a README
readme = """\
# MediSafe Companion (Modularized)

This is a modular split of your previous single-file Streamlit app.

## Structure
- `app.py`: Streamlit entry point (UI + kicks off LangGraph)
- `ddi_app/config.py`: env + OpenAI client
- `ddi_app/db_csv.py`: CSV DB helpers
- `ddi_app/web_retrieval.py`: httpx+bs4 web fetchers
- `ddi_app/llm_backoff.py`: LLM extraction fallback
- `ddi_app/neo4j_utils.py`: Neo4j helpers
- `ddi_app/pipeline.py`: LangGraph nodes, state, graph builder
- `ddi_app/ui.py`: Markdown formatting

## Run
```bash
pip install -r requirements.txt
cp .env.example .env  # and fill values
streamlit run app.py


